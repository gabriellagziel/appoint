groups:
  - name: app-oint-sli-slo-alerts
    rules:
      # ==============================================================================
      # SLI/SLO BASED ALERTS
      # ==============================================================================
      
      # 5xx Error Rate Alert - SLO: < 1% error rate over 3 minutes
      - alert: High5xxErrorRate
        expr: |
          (
            sum(rate(http_errors_5xx_rate[3m])) by (service) /
            sum(rate(http_requests_total[3m])) by (service)
          ) * 100 > 1
        for: 3m
        labels:
          severity: critical
          slo: error_rate
          team: devops
        annotations:
          summary: "High 5xx error rate detected for {{ $labels.service }}"
          description: "Service {{ $labels.service }} has a 5xx error rate of {{ $value }}% over the last 3 minutes, exceeding SLO of 1%"
          runbook_url: "https://docs.app-oint.com/runbooks/high-error-rate"
          dashboard_url: "http://grafana:3001/d/app-oint-overview"

      # 4xx Error Rate Alert - Warning at 5%
      - alert: High4xxErrorRate
        expr: |
          (
            sum(rate(http_errors_4xx_rate[5m])) by (service) /
            sum(rate(http_requests_total[5m])) by (service)
          ) * 100 > 5
        for: 5m
        labels:
          severity: warning
          team: devops
        annotations:
          summary: "High 4xx error rate detected for {{ $labels.service }}"
          description: "Service {{ $labels.service }} has a 4xx error rate of {{ $value }}% over the last 5 minutes"

      # Response Time P95 Alert - SLO: < 500ms
      - alert: HighResponseTimeP95
        expr: |
          http_request_duration_seconds{quantile="0.95"} * 1000 > 500
        for: 5m
        labels:
          severity: warning
          slo: latency
          team: devops
        annotations:
          summary: "High P95 response time for {{ $labels.service }}"
          description: "Service {{ $labels.service }} P95 response time is {{ $value }}ms, exceeding SLO of 500ms"

      # Response Time P99 Alert - SLO: < 1000ms
      - alert: HighResponseTimeP99
        expr: |
          http_request_duration_seconds{quantile="0.99"} * 1000 > 1000
        for: 3m
        labels:
          severity: critical
          slo: latency
          team: devops
        annotations:
          summary: "Very high P99 response time for {{ $labels.service }}"
          description: "Service {{ $labels.service }} P99 response time is {{ $value }}ms, exceeding SLO of 1000ms"

      # Traffic Drop Alert - 50% drop in traffic
      - alert: TrafficDropDetected
        expr: |
          (
            sum(rate(http_requests_total[5m])) by (service) <
            sum(rate(http_requests_total[1h] offset 1h)) by (service) * 0.5
          )
        for: 2m
        labels:
          severity: warning
          team: devops
        annotations:
          summary: "Sudden traffic drop detected for {{ $labels.service }}"
          description: "Service {{ $labels.service }} traffic has dropped by more than 50% compared to the same time 1 hour ago"

      # ==============================================================================
      # INFRASTRUCTURE ALERTS
      # ==============================================================================

      # Service Down Alert
      - alert: ServiceDown
        expr: up == 0
        for: 1m
        labels:
          severity: critical
          team: devops
        annotations:
          summary: "Service {{ $labels.instance }} is down"
          description: "Service {{ $labels.instance }} has been down for more than 1 minute"

      # High CPU Usage Alert
      - alert: HighCPUUsage
        expr: |
          (
            100 - (avg by (instance) (irate(node_cpu_seconds_total{mode="idle"}[5m])) * 100)
          ) > 80
        for: 5m
        labels:
          severity: warning
          team: devops
        annotations:
          summary: "High CPU usage on {{ $labels.instance }}"
          description: "CPU usage is {{ $value }}% on {{ $labels.instance }}"

      # High Memory Usage Alert
      - alert: HighMemoryUsage
        expr: |
          (
            (node_memory_MemTotal_bytes - node_memory_MemAvailable_bytes) / 
            node_memory_MemTotal_bytes
          ) * 100 > 75
        for: 5m
        labels:
          severity: warning
          team: devops
        annotations:
          summary: "High memory usage on {{ $labels.instance }}"
          description: "Memory usage is {{ $value }}% on {{ $labels.instance }}"

      # Critical Memory Usage Alert
      - alert: CriticalMemoryUsage
        expr: |
          (
            (node_memory_MemTotal_bytes - node_memory_MemAvailable_bytes) / 
            node_memory_MemTotal_bytes
          ) * 100 > 90
        for: 2m
        labels:
          severity: critical
          team: devops
        annotations:
          summary: "Critical memory usage on {{ $labels.instance }}"
          description: "Memory usage is {{ $value }}% on {{ $labels.instance }} - immediate attention required"

      # ==============================================================================
      # DATABASE ALERTS
      # ==============================================================================

      # Database Connection Issues
      - alert: DatabaseConnectionFailure
        expr: |
          increase(REDACTED_TOKEN[5m]) > 0
        for: 1m
        labels:
          severity: critical
          team: devops
        annotations:
          summary: "Database connection failures detected"
          description: "{{ $value }} database connection failures in the last 5 minutes"

      # Slow Database Queries
      - alert: SlowDatabaseQueries
        expr: |
          REDACTED_TOKEN{datname!="template0",datname!="template1"} > 300
        for: 3m
        labels:
          severity: warning
          team: devops
        annotations:
          summary: "Slow database queries detected"
          description: "Long running query detected ({{ $value }}s) on database {{ $labels.datname }}"

      # ==============================================================================
      # HEALTH CHECK ALERTS
      # ==============================================================================

      # Health Check Failure
      - alert: HealthCheckFailure
        expr: |
          health_check_status != 1
        for: 2m
        labels:
          severity: critical
          team: devops
        annotations:
          summary: "Health check failure for {{ $labels.service }}"
          description: "Health check for {{ $labels.service }} has been failing for more than 2 minutes"

      # Readiness Check Failure
      - alert: ReadinessCheckFailure
        expr: |
          readiness_check_status != 1
        for: 1m
        labels:
          severity: warning
          team: devops
        annotations:
          summary: "Readiness check failure for {{ $labels.service }}"
          description: "Readiness check for {{ $labels.service }} is failing"

      # ==============================================================================
      # REDIS ALERTS
      # ==============================================================================

      # Redis Connection Issues
      - alert: RedisConnectionFailure
        expr: |
          redis_up == 0
        for: 1m
        labels:
          severity: critical
          team: devops
        annotations:
          summary: "Redis connection failure"
          description: "Redis instance {{ $labels.instance }} is unreachable"

      # High Redis Memory Usage
      - alert: HighRedisMemoryUsage
        expr: |
          (redis_memory_used_bytes / redis_memory_max_bytes) * 100 > 80
        for: 5m
        labels:
          severity: warning
          team: devops
        annotations:
          summary: "High Redis memory usage"
          description: "Redis memory usage is {{ $value }}% on {{ $labels.instance }}"

      # ==============================================================================
      # DISK AND I/O ALERTS
      # ==============================================================================

      # High Disk Usage
      - alert: HighDiskUsage
        expr: |
          (
            (node_filesystem_size_bytes - node_filesystem_avail_bytes) /
            node_filesystem_size_bytes
          ) * 100 > 80
        for: 5m
        labels:
          severity: warning
          team: devops
        annotations:
          summary: "High disk usage on {{ $labels.instance }}"
          description: "Disk usage is {{ $value }}% on {{ $labels.instance }}:{{ $labels.mountpoint }}"

      # Critical Disk Usage
      - alert: CriticalDiskUsage
        expr: |
          (
            (node_filesystem_size_bytes - node_filesystem_avail_bytes) /
            node_filesystem_size_bytes
          ) * 100 > 95
        for: 2m
        labels:
          severity: critical
          team: devops
        annotations:
          summary: "Critical disk usage on {{ $labels.instance }}"
          description: "Disk usage is {{ $value }}% on {{ $labels.instance }}:{{ $labels.mountpoint }} - immediate attention required"

      # High I/O Wait
      - alert: HighIOWait
        expr: |
          avg by (instance) (irate(node_cpu_seconds_total{mode="iowait"}[5m])) * 100 > 20
        for: 5m
        labels:
          severity: warning
          team: devops
        annotations:
          summary: "High I/O wait on {{ $labels.instance }}"
          description: "I/O wait is {{ $value }}% on {{ $labels.instance }}"

  # ==============================================================================
  # REGIONAL FAILOVER ALERTS
  # ==============================================================================
  - name: app-oint-regional-alerts
    rules:
      # Regional Service Unavailability
      - alert: RegionalServiceUnavailable
        expr: |
          count by (region) (up{job=~"functions|dashboard"} == 0) >= 
          count by (region) (up{job=~"functions|dashboard"}) * 0.5
        for: 2m
        labels:
          severity: critical
          team: devops
          alert_type: regional_failure
        annotations:
          summary: "Regional service unavailability in {{ $labels.region }}"
          description: "More than 50% of services are down in region {{ $labels.region }}"
          action: "Trigger geo-DNS failover to secondary region"

      # Control Plane Loss
      - alert: ControlPlaneLoss
        expr: |
          absent(up{job="kubernetes-apiservers"}) or 
          count(up{job="kubernetes-apiservers"} == 1) < 2
        for: 3m
        labels:
          severity: critical
          team: devops
          alert_type: control_plane_failure
        annotations:
          summary: "Kubernetes control plane unavailable"
          description: "Control plane is unreachable or insufficient masters available"
          action: "Initiate regional failover procedures"

  # ==============================================================================
  # BUSINESS LOGIC ALERTS
  # ==============================================================================
  - name: app-oint-business-alerts
    rules:
      # Low Booking Success Rate
      - alert: LowBookingSuccessRate
        expr: |
          (
            sum(rate(booking_success_total[10m])) /
            sum(rate(booking_attempts_total[10m]))
          ) * 100 < 95
        for: 5m
        labels:
          severity: warning
          team: product
        annotations:
          summary: "Low booking success rate"
          description: "Booking success rate is {{ $value }}% over the last 10 minutes, below SLO of 95%"

      # Payment Processing Issues
      - alert: PaymentProcessingIssues
        expr: |
          increase(payment_errors_total[5m]) > 5
        for: 2m
        labels:
          severity: critical
          team: product
        annotations:
          summary: "Payment processing issues detected"
          description: "{{ $value }} payment errors in the last 5 minutes"

      # High Notification Delivery Failures
      - alert: HighNotificationFailures
        expr: |
          (
            sum(rate(notification_failures_total[5m])) /
            sum(rate(notification_attempts_total[5m]))
          ) * 100 > 10
        for: 3m
        labels:
          severity: warning
          team: product
        annotations:
          summary: "High notification delivery failure rate"
          description: "Notification failure rate is {{ $value }}% over the last 5 minutes"